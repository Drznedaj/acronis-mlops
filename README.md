# ğŸ“Š Churn Prediction ML Pipeline

A fully containerized end-to-end MLOps pipeline for customer churn prediction, featuring:

- ğŸ§  **Model Training & Tracking** with MLflow
- ğŸ¯ **Model Selection** & Registration
- ğŸš€ **Model Deployment** with FastAPI
- âš™ï¸ **Workflow Orchestration** via Apache Airflow
- ğŸ“¦ **Deployment** using Terraform and Docker
- ğŸ§ª **Serving** models via `/predict` API and storing `.pkl` artifacts
- ğŸ“ˆ **Basic Monitoring** via log-based latency/error tracking

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ airflow/                  # Airflow DAGs and configs
â”‚   â””â”€â”€ dags/churn_train_dag.py
|   â””â”€â”€ models/               # Pickled model artifacts (volume mounted)
â”œâ”€â”€ mlflow/                   # MLflow training script and Dockerfile
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ mlflow_model_server/      # FastAPI model server
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/                    # Training data (mounted into container)
â”‚   â””â”€â”€ telecom_customer_churn.csv
â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ main.tf              # Terraform config for all containers & volumes
â”œâ”€â”€ monitor_model_server.sh  # Monitoring script for latency & errors
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. âœ… Prerequisites

- [Docker](https://www.docker.com/)
- [Terraform](https://developer.hashicorp.com/terraform/downloads)
- Python 3.12+ (for local tests)

---

### 2. ğŸ› ï¸ Run the Full System

```bash
cd terraform/
terraform init
terraform apply --auto-approve
```

This will spin up:

- MLflow tracking server (`localhost:5001`)
- Model API server (`localhost:8000`)
- Local Docker registry (`localhost:5050`)

Then spin up Airflow with:

```bash
docker compose up -d
```

---

### 3. ğŸ”„ Run the DAG

Go to Airflow at [http://localhost:8080](http://localhost:8080):

- Trigger the `churn_train_mlflow_dag`
- It will:
  - Train models for different datasets
  - Log metrics, plots, and artifacts to MLflow
  - Pick the best model
  - Register and promote it to "Production"
  - Save it as a `.pkl` in `/model_pickles`
  - Update `best_model.pkl` for serving

---

## ğŸ”¬ Predicting with FastAPI

```bash
curl -X POST http://localhost:8000/predict   -H "Content-Type: application/json"   -d @mlflow_model_server input_example.json
```

### âœ… Example Response

```json
{
  "predictions": [0]
}
```

---

## ğŸ“¦ Artifacts & Model Registry

- MLflow UI: [http://localhost:5001](http://localhost:5001)
- Registered models: `telecom_churn_model`
- Pickled models stored at `./models/`

---

## ğŸ” API Endpoints

| Route         | Method | Description                      |
|---------------|--------|----------------------------------|
| `/predict`    | POST   | Predict from input JSON          |

---

## ğŸ“ˆ Basic Monitoring

App logs prediction latency and errors to `/models/model_server.log`.

Run the included script:

```bash
./monitor_model_server.sh
```

Youâ€™ll get output like:

```bash
ğŸ“Š Model Server Monitoring Report
ğŸ”¹ Total requests this hour: 48
â±ï¸  Average prediction latency: 113ms
âŒ Prediction errors this hour: 1
âš ï¸  Alerts:
 - All clear âœ…
```

---

## ğŸ§¼ Cleaning Up

To stop and remove everything:

```bash
terraform destroy --auto-approve
```

And for Airflow:

```bash
docker compose down
```

To reset local registry or volumes:

```bash
docker volume rm $(docker volume ls -q | grep model)
docker rm -f local_registry mlflow_server model_api
```
